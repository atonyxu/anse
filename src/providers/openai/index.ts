import {
  handlePrompt,
  handleRapidPrompt,
} from './handler'
import type { Provider } from '@/types/provider'

const providerOpenAI = () => {
  const provider: Provider = {
    id: 'provider-openai',
    icon: 'i-simple-icons-openai', // @unocss-include
    name: 'OpenAI',
    globalSettings: [
      {
        key: 'apiKey',
        name: 'API Key',
        type: 'api-key',
      },
      {
        key: 'baseUrl',
        name: 'Base URL',
        description: 'Custom base url for OpenAI API.',
        type: 'input',
        default: 'https://api.openai.com',
      },
      {
        key: 'model',
        name: 'OpenAI model',
        description: 'Custom gpt model for OpenAI API.',
        type: 'select',
        options: [
          { value: 'gpt-3.5-turbo', label: 'gpt-3.5-turbo' },
          { value: 'gpt-4', label: 'gpt-4' },
          { value: 'gpt-4-0314', label: 'gpt-4-0314' },
          { value: 'gpt-4-0613', label: 'gpt-4-0613' },
          { value: 'gpt-4-1106-preview', label: 'gpt-4-1106-preview' },
          { value: 'gpt-4-0125-preview', label: 'gpt-4-0125-preview' },
          { value: 'gpt-4-turbo-preview', label: 'gpt-4-turbo-preview' },
          { value: 'gpt-4-32k', label: 'gpt-4-32k' },
          { value: 'gpt-4-32k-0314', label: 'gpt-4-32k-0314' },
          { value: 'gpt-4-32k-0613', label: 'gpt-4-32k-0613' },
          { value: 'gpt-3.5-turbo-0125', label: 'gpt-3.5-turbo-0125' },
          { value: 'gpt-3.5-turbo-0301', label: 'gpt-3.5-turbo-0301' },
          { value: 'gpt-3.5-turbo-0613', label: 'gpt-3.5-turbo-0613' },
          { value: 'gpt-3.5-turbo-1106', label: 'gpt-3.5-turbo-1106' },
          { value: 'gpt-3.5-turbo-16k', label: 'gpt-3.5-turbo-16k' },
          { value: 'gpt-3.5-turbo-16k-0613', label: 'gpt-3.5-turbo-16k-0613' },
          { value: '@cf/meta/llama-3-8b-instruct', label: '@cf/meta/llama-3-8b-instruct' },
          { value: '@cf/meta/llama-2-7b-chat-fp16', label: '@cf/meta/llama-2-7b-chat-fp16' },
          { value: '@cf/mistral/mistral-7b-instruct-v0.1', label: '@cf/mistral/mistral-7b-instruct-v0.1' },
          { value: '@cf/meta/llama-2-7b-chat-int8', label: '@cf/meta/llama-2-7b-chat-int8' },
          { value: '@cf/qwen/qwen1.5-0.5b-chat', label: '@cf/qwen/qwen1.5-0.5b-chat' },
          { value: '@cf/google/gemma-2b-it-lora', label: '@cf/google/gemma-2b-it-lora' },
          { value: '@hf/nexusflow/starling-lm-7b-beta', label: '@hf/nexusflow/starling-lm-7b-beta' },
          { value: '@hf/thebloke/llamaguard-7b-awq', label: '@hf/thebloke/llamaguard-7b-awq' },
          { value: '@hf/thebloke/neural-chat-7b-v3-1-awq', label: '@hf/thebloke/neural-chat-7b-v3-1-awq' },
          { value: '@cf/mistral/mistral-7b-instruct-v0.2-lora', label: '@cf/mistral/mistral-7b-instruct-v0.2-lora' },
          { value: '@cf/tinyllama/tinyllama-1.1b-chat-v1.0', label: '@cf/tinyllama/tinyllama-1.1b-chat-v1.0' },
          { value: '@hf/mistral/mistral-7b-instruct-v0.2', label: '@hf/mistral/mistral-7b-instruct-v0.2' },
          { value: '@hf/thebloke/codellama-7b-instruct-awq', label: '@hf/thebloke/codellama-7b-instruct-awq' },
          { value: '@hf/mistral/mistral-7b-instruct-v0.2', label: '@hf/mistral/mistral-7b-instruct-v0.2' },
          { value: '@cf/thebloke/discolm-german-7b-v1-awq', label: '@cf/thebloke/discolm-german-7b-v1-awq' },
          { value: '@hf/thebloke/mistral-7b-instruct-v0.1-awq', label: '@hf/thebloke/mistral-7b-instruct-v0.1-awq' },
          { value: '@hf/thebloke/openchat_3.5-awq', label: '@hf/thebloke/openchat_3.5-awq' },
          { value: '@cf/qwen/qwen1.5-7b-chat-awq', label: '@cf/qwen/qwen1.5-7b-chat-awq' },
          { value: '@hf/thebloke/llama-2-13b-chat-awq', label: '@hf/thebloke/llama-2-13b-chat-awq' },
          { value: '@hf/thebloke/deepseek-coder-6.7b-base-awq', label: '@hf/thebloke/deepseek-coder-6.7b-base-awq' },
          { value: '@cf/meta-llama/llama-2-7b-chat-hf-lora', label: '@cf/meta-llama/llama-2-7b-chat-hf-lora' },
          { value: '@hf/thebloke/openhermes-2.5-mistral-7b-awq', label: '@hf/thebloke/openhermes-2.5-mistral-7b-awq' },
          { value: '@hf/thebloke/deepseek-coder-6.7b-instruct-awq', label: '@hf/thebloke/deepseek-coder-6.7b-instruct-awq' },
          { value: '@cf/deepseek-ai/deepseek-math-7b-instruct', label: '@cf/deepseek-ai/deepseek-math-7b-instruct' },
          { value: '@cf/tiiuae/falcon-7b-instruct', label: '@cf/tiiuae/falcon-7b-instruct' },
          { value: '@hf/nousresearch/hermes-2-pro-mistral-7b', label: '@hf/nousresearch/hermes-2-pro-mistral-7b' },
          { value: '@hf/thebloke/zephyr-7b-beta-awq', label: '@hf/thebloke/zephyr-7b-beta-awq' },
          { value: '@cf/google/gemma-7b-it-lora', label: '@cf/google/gemma-7b-it-lora' },
          { value: '@cf/qwen/qwen1.5-1.8b-chat', label: '@cf/qwen/qwen1.5-1.8b-chat' },
          { value: '@cf/defog/sqlcoder-7b-2', label: '@cf/defog/sqlcoder-7b-2' },
          { value: '@cf/microsoft/phi-2', label: '@cf/microsoft/phi-2' },
          { value: '@hf/google/gemma-7b-it', label: '@hf/google/gemma-7b-it' },
          { value: '@cf/qwen/qwen1.5-14b-chat-awq', label: '@cf/qwen/qwen1.5-14b-chat-awq' },
          { value: '@cf/openchat/openchat-3.5-0106', label: '@cf/openchat/openchat-3.5-0106' },
        ],
        default: 'gpt-3.5-turbo',
      },
      {
        key: 'maxTokens',
        name: 'Max Tokens',
        description: 'The maximum number of tokens to generate in the completion.',
        type: 'slider',
        min: 0,
        max: 32768,
        default: 2048,
        step: 1,
      },
      {
        key: 'messageHistorySize',
        name: 'Max History Message Size',
        description: 'The number of retained historical messages will be truncated if the length of the message exceeds the MaxToken parameter.',
        type: 'slider',
        min: 1,
        max: 24,
        default: 5,
        step: 1,
      },
      {
        key: 'temperature',
        name: 'Temperature',
        type: 'slider',
        description: 'What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.',
        min: 0,
        max: 2,
        default: 0.7,
        step: 0.01,
      },
      {
        key: 'top_p',
        name: 'Top P',
        description: 'An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.',
        type: 'slider',
        min: 0,
        max: 1,
        default: 1,
        step: 0.01,
      },
    ],
    bots: [
      {
        id: 'chat_continuous',
        type: 'chat_continuous',
        name: 'Continuous Chat',
        settings: [],
      },
      {
        id: 'chat_single',
        type: 'chat_single',
        name: 'Single Chat',
        settings: [],
      },
      {
        id: 'image_generation',
        type: 'image_generation',
        name: 'DALLÂ·E',
        settings: [],
      },
    ],
    handlePrompt,
    handleRapidPrompt,
  }
  return provider
}

export default providerOpenAI
